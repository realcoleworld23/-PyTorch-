{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: swanlab version 0.6.5 is available!  Upgrade: `pip install -U swanlab`    \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.1                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/swanlog/run-20250707_154934-d779159b\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üëã Hi \u001b[1m\u001b[39mSZY_230507\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mdog-12\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üè† View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/bdbvti0qailz65eqx85d0\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/bdbvti0qailz65eqx85d0\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2326/3683885626.py:85: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.2),\n",
      "/tmp/ipykernel_2326/3683885626.py:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1 Train:   0%|          | 0/507 [00:00<?, ?it/s]/tmp/ipykernel_2326/3683885626.py:187: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [02:10<00:00,  3.89it/s]\n",
      "Epoch 1 Val:   0%|          | 0/63 [00:00<?, ?it/s]/tmp/ipykernel_2326/3683885626.py:204: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:11<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1/30 | TL 0.766 TA 0.849 | VL 0.910 VA 0.879\n",
      "  ‚úî Save best 0.879407001370375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:45<00:00,  4.78it/s]\n",
      "Epoch 2 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2/30 | TL 0.781 TA 0.820 | VL 0.900 VA 0.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:45<00:00,  4.78it/s]\n",
      "Epoch 3 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E3/30 | TL 0.762 TA 0.844 | VL 0.918 VA 0.878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:45<00:00,  4.78it/s]\n",
      "Epoch 4 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4/30 | TL 0.803 TA 0.830 | VL 0.902 VA 0.880\n",
      "  ‚úî Save best 0.8799053195465305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:46<00:00,  4.78it/s]\n",
      "Epoch 5 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5/30 | TL 0.785 TA 0.808 | VL 0.905 VA 0.880\n",
      "  ‚úî Save best 0.8800298990905693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:46<00:00,  4.76it/s]\n",
      "Epoch 6 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E6/30 | TL 0.791 TA 0.822 | VL 0.893 VA 0.881\n",
      "  ‚úî Save best 0.8811511149869191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:46<00:00,  4.78it/s]\n",
      "Epoch 7 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E7/30 | TL 0.765 TA 0.825 | VL 0.903 VA 0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:45<00:00,  4.79it/s]\n",
      "Epoch 8 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E8/30 | TL 0.731 TA 0.840 | VL 0.900 VA 0.882\n",
      "  ‚úî Save best 0.8818985922511524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:46<00:00,  4.78it/s]\n",
      "Epoch 9 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E9/30 | TL 0.746 TA 0.845 | VL 0.896 VA 0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:46<00:00,  4.77it/s]\n",
      "Epoch 10 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E10/30 | TL 0.739 TA 0.859 | VL 0.905 VA 0.884\n",
      "  ‚úî Save best 0.8842656035878909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [01:46<00:00,  4.76it/s]\n",
      "Epoch 11 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:07<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E11/30 | TL 0.787 TA 0.830 | VL 0.905 VA 0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train:  41%|‚ñà‚ñà‚ñà‚ñà      | 207/507 [00:44<01:04,  4.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 226\u001b[39m\n\u001b[32m    223\u001b[39m     torch.save(model.state_dict(), \u001b[33m\"\u001b[39m\u001b[33m/workspace/last_se_resnet50_mushroom10.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    192\u001b[39m         loss = lam*criterion(logits, y_a) + (\u001b[32m1\u001b[39m-lam)*criterion(logits, y_b)\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m scaler.scale(loss).backward(); \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m; scaler.update()\n\u001b[32m    194\u001b[39m tl += loss.item()*xb.size(\u001b[32m0\u001b[39m)\n\u001b[32m    195\u001b[39m preds = logits.argmax(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/amp/grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import swanlab\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "swanlab.init(project=\"mushroom-toxicity-detection\", run=\"se_cbam_resnet50_mushroom1\")\n",
    "\n",
    "# ---------------------------- Âü∫Á°ÄÈÖçÁΩÆ ----------------------------\n",
    "PROJECT = \"mushroom-toxicity-detection\"\n",
    "RUN_NAME = \"se_cbam_resnet50_v100\"\n",
    "DATA_DIR = Path(\"/workspace/mushroom_dataset_single_split\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 8\n",
    "MIXUP_ALPHA = 0.4\n",
    "MIXUP_PROB = 0.3\n",
    "TTA_SCALES = [224, 256]\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "# ---------------------------- SE ResNet50 + Dropout ----------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    def __init__(self, bottleneck):\n",
    "        super().__init__()\n",
    "        self.body = bottleneck\n",
    "        self.se = SEBlock(bottleneck.conv3.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.se(self.body(x))\n",
    "\n",
    "def build_backbone(num_classes):\n",
    "    m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    for name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        setattr(m, name, nn.Sequential(*[SEBottleneck(b) for b in getattr(m, name)]))\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(2048, num_classes)\n",
    "    )\n",
    "    # ÂÜªÁªìÂâç‰∏§Â±Ç\n",
    "    for param in m.layer1.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in m.layer2.parameters():\n",
    "        param.requires_grad = True\n",
    "    return m\n",
    "\n",
    "# ---------------------------- Êï∞ÊçÆÂ¢ûÂº∫ ----------------------------\n",
    "train_tf = A.Compose([\n",
    "    A.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.HueSaturationValue(10, 15, 10, p=0.5),\n",
    "    A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
    "    A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.2),\n",
    "    A.GaussianBlur(5, p=0.4),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# ---------------------------- Dataset ----------------------------\n",
    "class AlbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, tf):\n",
    "        self.ds = datasets.ImageFolder(root)\n",
    "        self.tf = tf\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        p, l = self.ds.samples[idx]\n",
    "        img = np.asarray(Image.open(p).convert(\"RGB\"))\n",
    "        return self.tf(image=img)[\"image\"], l\n",
    "\n",
    "train_ds = AlbDataset(DATA_DIR/\"train\", train_tf)\n",
    "val_ds = AlbDataset(DATA_DIR/\"val\", val_tf)\n",
    "num_classes = len(train_ds.ds.classes)\n",
    "\n",
    "# -------- WeightedRandomSampler --------\n",
    "labels = [l for _, l in train_ds.ds.samples]\n",
    "counts = np.bincount(labels)\n",
    "weights = 1.0 / counts\n",
    "sample_weights = [weights[l] for l in labels]\n",
    "train_sampler = WeightedRandomSampler(sample_weights, len(train_ds), replacement=True)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                    num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------- MixUp ----------------------------\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ---------------------------- Label Smoothing Loss ----------------------------\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logprobs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * logprobs, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingLoss(num_classes, smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# ---------------------------- ‰ºòÂåñÂô® & Ë∞ÉÂ∫¶ ----------------------------\n",
    "model = build_backbone(num_classes).to(DEVICE)\n",
    "opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-4)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler = GradScaler()\n",
    "\n",
    "class EarlyStop:\n",
    "    def __init__(self, p=7, delta=0.001):\n",
    "        self.p, self.d = p, delta; self.best = 1e9; self.c = 0\n",
    "    def __call__(self, v):\n",
    "        if v < self.best - self.d: self.best, self.c = v, 0\n",
    "        else: self.c += 1\n",
    "        return self.c >= self.p\n",
    "\n",
    "estop = EarlyStop()\n",
    "\n",
    "# Âä†ËΩΩ‰πãÂâç‰øùÂ≠òÁöÑÊúÄ‰ºòÊ®°Âûã\n",
    "best_model_path = \"/workspace/best_se_resnet50_mushroom9.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    \n",
    "# ---------------------------- ËÆ≠ÁªÉÂæ™ÁéØ ----------------------------\n",
    "def train():\n",
    "    best = 0\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tl, tc = 0, 0\n",
    "        train_pbar = tqdm(train_ld, desc=f\"Epoch {ep} Train\", unit=\"it\", dynamic_ncols=True)\n",
    "        for xb, yb in train_pbar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            lam = 1.0\n",
    "            if random.random() < MIXUP_PROB:\n",
    "                xb, y_a, y_b, lam = mixup_data(xb, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                logits = model(xb)\n",
    "                if lam == 1.0:\n",
    "                    loss = criterion(logits, yb)\n",
    "                else:\n",
    "                    loss = lam*criterion(logits, y_a) + (1-lam)*criterion(logits, y_b)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            tl += loss.item()*xb.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            tc += (preds==yb).sum().item()\n",
    "        train_loss = tl/len(train_ds); train_acc = tc/len(train_ds)\n",
    "\n",
    "        model.eval(); vl, vc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_ld, desc=f\"Epoch {ep} Val\", unit=\"it\", dynamic_ncols=True)\n",
    "            for xb, yb in val_pbar:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                vl += loss.item()*xb.size(0)\n",
    "\n",
    "\n",
    "                vc += (logits.argmax(1)== yb).sum().item()\n",
    "        val_loss = vl/len(val_ds); val_acc = vc/len(val_ds)\n",
    "        sched.step()\n",
    "\n",
    "        print(f\"E{ep}/{EPOCHS} | TL {train_loss:.3f} TA {train_acc:.3f} | VL {val_loss:.3f} VA {val_acc:.3f}\")\n",
    "        swanlab.log({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc,\"lr\":sched.get_last_lr()[0]})\n",
    "\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "            torch.save(model.state_dict(), \"/workspace/best_se_resnet50_mushroom10.pth\")\n",
    "            print(\"  ‚úî Save best\", best)\n",
    "        if estop(val_loss):\n",
    "            print(\"Early stop!\"); break\n",
    "    torch.save(model.state_dict(), \"/workspace/last_se_resnet50_mushroom10.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: swanlab version 0.6.5 is available!  Upgrade: `pip install -U swanlab`    \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.1                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/swanlog/run-20250707_161220-c5c1effa\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üëã Hi \u001b[1m\u001b[39mSZY_230507\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mdog-12\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üè† View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/3jbm15i658ielbe1p7gm4\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/3jbm15i658ielbe1p7gm4\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2326/2780844363.py:85: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.2),\n",
      "/tmp/ipykernel_2326/2780844363.py:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1 Train:   0%|          | 0/338 [00:00<?, ?it/s]/tmp/ipykernel_2326/2780844363.py:187: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338/338 [01:45<00:00,  3.20it/s]\n",
      "Epoch 1 Val:   0%|          | 0/42 [00:00<?, ?it/s]/tmp/ipykernel_2326/2780844363.py:204: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:07<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1/30 | TL 0.695 TA 0.870 | VL 0.864 VA 0.893\n",
      "  ‚úî Save best 0.8929861716706117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338/338 [01:45<00:00,  3.20it/s]\n",
      "Epoch 2 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:07<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2/30 | TL 0.677 TA 0.847 | VL 0.873 VA 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338/338 [01:45<00:00,  3.20it/s]\n",
      "Epoch 3 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:07<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E3/30 | TL 0.674 TA 0.875 | VL 0.875 VA 0.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338/338 [01:45<00:00,  3.20it/s]\n",
      "Epoch 4 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:07<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4/30 | TL 0.726 TA 0.855 | VL 0.880 VA 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338/338 [01:45<00:00,  3.20it/s]\n",
      "Epoch 5 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:07<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5/30 | TL 0.723 TA 0.882 | VL 0.882 VA 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338/338 [01:45<00:00,  3.19it/s]\n",
      "Epoch 6 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:07<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E6/30 | TL 0.686 TA 0.877 | VL 0.880 VA 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338/338 [01:45<00:00,  3.21it/s]\n",
      "Epoch 7 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:07<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E7/30 | TL 0.675 TA 0.879 | VL 0.884 VA 0.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338/338 [01:45<00:00,  3.20it/s]\n",
      "Epoch 8 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:07<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E8/30 | TL 0.643 TA 0.866 | VL 0.881 VA 0.892\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import swanlab\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "swanlab.init(project=\"mushroom-toxicity-detection\", run=\"se_cbam_resnet50_mushroom1\")\n",
    "\n",
    "# ---------------------------- Âü∫Á°ÄÈÖçÁΩÆ ----------------------------\n",
    "PROJECT = \"mushroom-toxicity-detection\"\n",
    "RUN_NAME = \"se_cbam_resnet50_v100\"\n",
    "DATA_DIR = Path(\"/workspace/mushroom_dataset_single_split\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 192\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 8\n",
    "MIXUP_ALPHA = 0.35\n",
    "MIXUP_PROB = 0.25\n",
    "TTA_SCALES = [224, 256]\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "# ---------------------------- SE ResNet50 + Dropout ----------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    def __init__(self, bottleneck):\n",
    "        super().__init__()\n",
    "        self.body = bottleneck\n",
    "        self.se = SEBlock(bottleneck.conv3.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.se(self.body(x))\n",
    "\n",
    "def build_backbone(num_classes):\n",
    "    m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    for name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        setattr(m, name, nn.Sequential(*[SEBottleneck(b) for b in getattr(m, name)]))\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(2048, num_classes)\n",
    "    )\n",
    "    # ÂÜªÁªìÂâç‰∏§Â±Ç\n",
    "    for param in m.layer1.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in m.layer2.parameters():\n",
    "        param.requires_grad = True\n",
    "    return m\n",
    "\n",
    "# ---------------------------- Êï∞ÊçÆÂ¢ûÂº∫ ----------------------------\n",
    "train_tf = A.Compose([\n",
    "    A.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.HueSaturationValue(10, 15, 10, p=0.5),\n",
    "    A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
    "    A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.2),\n",
    "    A.GaussianBlur(5, p=0.4),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# ---------------------------- Dataset ----------------------------\n",
    "class AlbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, tf):\n",
    "        self.ds = datasets.ImageFolder(root)\n",
    "        self.tf = tf\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        p, l = self.ds.samples[idx]\n",
    "        img = np.asarray(Image.open(p).convert(\"RGB\"))\n",
    "        return self.tf(image=img)[\"image\"], l\n",
    "\n",
    "train_ds = AlbDataset(DATA_DIR/\"train\", train_tf)\n",
    "val_ds = AlbDataset(DATA_DIR/\"val\", val_tf)\n",
    "num_classes = len(train_ds.ds.classes)\n",
    "\n",
    "# -------- WeightedRandomSampler --------\n",
    "labels = [l for _, l in train_ds.ds.samples]\n",
    "counts = np.bincount(labels)\n",
    "weights = 1.0 / counts\n",
    "sample_weights = [weights[l] for l in labels]\n",
    "train_sampler = WeightedRandomSampler(sample_weights, len(train_ds), replacement=True)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                    num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------- MixUp ----------------------------\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ---------------------------- Label Smoothing Loss ----------------------------\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logprobs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * logprobs, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingLoss(num_classes, smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# ---------------------------- ‰ºòÂåñÂô® & Ë∞ÉÂ∫¶ ----------------------------\n",
    "model = build_backbone(num_classes).to(DEVICE)\n",
    "opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5, weight_decay=1e-4)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler = GradScaler()\n",
    "\n",
    "class EarlyStop:\n",
    "    def __init__(self, p=7, delta=0.001):\n",
    "        self.p, self.d = p, delta; self.best = 1e9; self.c = 0\n",
    "    def __call__(self, v):\n",
    "        if v < self.best - self.d: self.best, self.c = v, 0\n",
    "        else: self.c += 1\n",
    "        return self.c >= self.p\n",
    "\n",
    "estop = EarlyStop()\n",
    "\n",
    "# Âä†ËΩΩ‰πãÂâç‰øùÂ≠òÁöÑÊúÄ‰ºòÊ®°Âûã\n",
    "best_model_path = \"/workspace/best_se_resnet50_mushroom8.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    \n",
    "# ---------------------------- ËÆ≠ÁªÉÂæ™ÁéØ ----------------------------\n",
    "def train():\n",
    "    best = 0\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tl, tc = 0, 0\n",
    "        train_pbar = tqdm(train_ld, desc=f\"Epoch {ep} Train\", unit=\"it\", dynamic_ncols=True)\n",
    "        for xb, yb in train_pbar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            lam = 1.0\n",
    "            if random.random() < MIXUP_PROB:\n",
    "                xb, y_a, y_b, lam = mixup_data(xb, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                logits = model(xb)\n",
    "                if lam == 1.0:\n",
    "                    loss = criterion(logits, yb)\n",
    "                else:\n",
    "                    loss = lam*criterion(logits, y_a) + (1-lam)*criterion(logits, y_b)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            tl += loss.item()*xb.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            tc += (preds==yb).sum().item()\n",
    "        train_loss = tl/len(train_ds); train_acc = tc/len(train_ds)\n",
    "\n",
    "        model.eval(); vl, vc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_ld, desc=f\"Epoch {ep} Val\", unit=\"it\", dynamic_ncols=True)\n",
    "            for xb, yb in val_pbar:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                vl += loss.item()*xb.size(0)\n",
    "\n",
    "\n",
    "                vc += (logits.argmax(1)== yb).sum().item()\n",
    "        val_loss = vl/len(val_ds); val_acc = vc/len(val_ds)\n",
    "        sched.step()\n",
    "\n",
    "        print(f\"E{ep}/{EPOCHS} | TL {train_loss:.3f} TA {train_acc:.3f} | VL {val_loss:.3f} VA {val_acc:.3f}\")\n",
    "        swanlab.log({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc,\"lr\":sched.get_last_lr()[0]})\n",
    "\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "            torch.save(model.state_dict(), \"/workspace/best_se_resnet50_mushroom10.pth\")\n",
    "            print(\"  ‚úî Save best\", best)\n",
    "        if estop(val_loss):\n",
    "            print(\"Early stop!\"); break\n",
    "    torch.save(model.state_dict(), \"/workspace/last_se_resnet50_mushroom10.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: swanlab version 0.6.5 is available!  Upgrade: `pip install -U swanlab`    \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.1                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/swanlog/run-20250707_163655-eccd019e\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üëã Hi \u001b[1m\u001b[39mSZY_230507\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mpig-13\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üè† View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/x47544ky0ok1ra44y32bt\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/x47544ky0ok1ra44y32bt\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2326/3115912640.py:85: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.1),\n",
      "/tmp/ipykernel_2326/3115912640.py:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1 Train:   0%|          | 0/290 [00:00<?, ?it/s]/tmp/ipykernel_2326/3115912640.py:187: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:46<00:00,  2.72it/s]\n",
      "Epoch 1 Val:   0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_2326/3115912640.py:204: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1/30 | TL 0.705 TA 0.862 | VL 0.857 VA 0.893\n",
      "  ‚úî Save best 0.8933599103027283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 2 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2/30 | TL 0.703 TA 0.846 | VL 0.862 VA 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 3 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E3/30 | TL 0.689 TA 0.877 | VL 0.857 VA 0.894\n",
      "  ‚úî Save best 0.8939828080229226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 4 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4/30 | TL 0.661 TA 0.852 | VL 0.859 VA 0.895\n",
      "  ‚úî Save best 0.8946057057431169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 5 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5/30 | TL 0.690 TA 0.884 | VL 0.858 VA 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 6 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E6/30 | TL 0.679 TA 0.831 | VL 0.858 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 7 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E7/30 | TL 0.639 TA 0.913 | VL 0.861 VA 0.895\n",
      "  ‚úî Save best 0.8948548648311947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:44<00:00,  2.76it/s]\n",
      "Epoch 8 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E8/30 | TL 0.652 TA 0.884 | VL 0.858 VA 0.894\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import swanlab\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "swanlab.init(project=\"mushroom-toxicity-detection\", run=\"se_cbam_resnet50_mushroom1\")\n",
    "\n",
    "# ---------------------------- Âü∫Á°ÄÈÖçÁΩÆ ----------------------------\n",
    "PROJECT = \"mushroom-toxicity-detection\"\n",
    "RUN_NAME = \"se_cbam_resnet50_v100\"\n",
    "DATA_DIR = Path(\"/workspace/mushroom_dataset_single_split\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 224\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 8\n",
    "MIXUP_ALPHA = 0.35\n",
    "MIXUP_PROB = 0.25\n",
    "TTA_SCALES = [224, 256]\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "# ---------------------------- SE ResNet50 + Dropout ----------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    def __init__(self, bottleneck):\n",
    "        super().__init__()\n",
    "        self.body = bottleneck\n",
    "        self.se = SEBlock(bottleneck.conv3.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.se(self.body(x))\n",
    "\n",
    "def build_backbone(num_classes):\n",
    "    m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    for name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        setattr(m, name, nn.Sequential(*[SEBottleneck(b) for b in getattr(m, name)]))\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(2048, num_classes)\n",
    "    )\n",
    "    # ÂÜªÁªìÂâç‰∏§Â±Ç\n",
    "    for param in m.layer1.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in m.layer2.parameters():\n",
    "        param.requires_grad = True\n",
    "    return m\n",
    "\n",
    "# ---------------------------- Êï∞ÊçÆÂ¢ûÂº∫ ----------------------------\n",
    "train_tf = A.Compose([\n",
    "    A.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.HueSaturationValue(10, 15, 10, p=0.5),\n",
    "    A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
    "    A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.1),\n",
    "    A.GaussianBlur(5, p=0.4),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# ---------------------------- Dataset ----------------------------\n",
    "class AlbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, tf):\n",
    "        self.ds = datasets.ImageFolder(root)\n",
    "        self.tf = tf\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        p, l = self.ds.samples[idx]\n",
    "        img = np.asarray(Image.open(p).convert(\"RGB\"))\n",
    "        return self.tf(image=img)[\"image\"], l\n",
    "\n",
    "train_ds = AlbDataset(DATA_DIR/\"train\", train_tf)\n",
    "val_ds = AlbDataset(DATA_DIR/\"val\", val_tf)\n",
    "num_classes = len(train_ds.ds.classes)\n",
    "\n",
    "# -------- WeightedRandomSampler --------\n",
    "labels = [l for _, l in train_ds.ds.samples]\n",
    "counts = np.bincount(labels)\n",
    "weights = 1.0 / counts\n",
    "sample_weights = [weights[l] for l in labels]\n",
    "train_sampler = WeightedRandomSampler(sample_weights, len(train_ds), replacement=True)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                    num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------- MixUp ----------------------------\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ---------------------------- Label Smoothing Loss ----------------------------\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logprobs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * logprobs, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingLoss(num_classes, smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# ---------------------------- ‰ºòÂåñÂô® & Ë∞ÉÂ∫¶ ----------------------------\n",
    "model = build_backbone(num_classes).to(DEVICE)\n",
    "opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=1e-4)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler = GradScaler()\n",
    "\n",
    "class EarlyStop:\n",
    "    def __init__(self, p=7, delta=0.001):\n",
    "        self.p, self.d = p, delta; self.best = 1e9; self.c = 0\n",
    "    def __call__(self, v):\n",
    "        if v < self.best - self.d: self.best, self.c = v, 0\n",
    "        else: self.c += 1\n",
    "        return self.c >= self.p\n",
    "\n",
    "estop = EarlyStop()\n",
    "\n",
    "# Âä†ËΩΩ‰πãÂâç‰øùÂ≠òÁöÑÊúÄ‰ºòÊ®°Âûã\n",
    "best_model_path = \"/workspace/best_se_resnet50_mushroom10.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    \n",
    "# ---------------------------- ËÆ≠ÁªÉÂæ™ÁéØ ----------------------------\n",
    "def train():\n",
    "    best = 0\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tl, tc = 0, 0\n",
    "        train_pbar = tqdm(train_ld, desc=f\"Epoch {ep} Train\", unit=\"it\", dynamic_ncols=True)\n",
    "        for xb, yb in train_pbar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            lam = 1.0\n",
    "            if random.random() < MIXUP_PROB:\n",
    "                xb, y_a, y_b, lam = mixup_data(xb, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                logits = model(xb)\n",
    "                if lam == 1.0:\n",
    "                    loss = criterion(logits, yb)\n",
    "                else:\n",
    "                    loss = lam*criterion(logits, y_a) + (1-lam)*criterion(logits, y_b)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            tl += loss.item()*xb.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            tc += (preds==yb).sum().item()\n",
    "        train_loss = tl/len(train_ds); train_acc = tc/len(train_ds)\n",
    "\n",
    "        model.eval(); vl, vc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_ld, desc=f\"Epoch {ep} Val\", unit=\"it\", dynamic_ncols=True)\n",
    "            for xb, yb in val_pbar:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                vl += loss.item()*xb.size(0)\n",
    "\n",
    "\n",
    "                vc += (logits.argmax(1)== yb).sum().item()\n",
    "        val_loss = vl/len(val_ds); val_acc = vc/len(val_ds)\n",
    "        sched.step()\n",
    "\n",
    "        print(f\"E{ep}/{EPOCHS} | TL {train_loss:.3f} TA {train_acc:.3f} | VL {val_loss:.3f} VA {val_acc:.3f}\")\n",
    "        swanlab.log({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc,\"lr\":sched.get_last_lr()[0]})\n",
    "\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "            torch.save(model.state_dict(), \"/workspace/best_se_resnet50_mushroom11.pth\")\n",
    "            print(\"  ‚úî Save best\", best)\n",
    "        if estop(val_loss):\n",
    "            print(\"Early stop!\"); break\n",
    "    torch.save(model.state_dict(), \"/workspace/last_se_resnet50_mushroom11.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: \\ Waiting for the swanlab cloud response."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: swanlab version 0.6.5 is available!  Upgrade: `pip install -U swanlab`    \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.1                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/swanlog/run-20250707_170453-6e788643\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üëã Hi \u001b[1m\u001b[39mSZY_230507\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mcat-14\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üè† View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/2ue53lgkqhtziaeb0m0je\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/2ue53lgkqhtziaeb0m0je\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2326/1764352669.py:85: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.05),\n",
      "/tmp/ipykernel_2326/1764352669.py:166: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1 Train:   0%|          | 0/290 [00:00<?, ?it/s]/tmp/ipykernel_2326/1764352669.py:195: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 1 Val:   0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_2326/1764352669.py:212: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1/30 | TL 0.419 TA 0.834 | VL 0.638 VA 0.894\n",
      "  ‚úî Save best 0.8942319671110004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 2 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2/30 | TL 0.417 TA 0.871 | VL 0.640 VA 0.895\n",
      "  ‚úî Save best 0.8948548648311947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 3 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E3/30 | TL 0.381 TA 0.895 | VL 0.641 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 4 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4/30 | TL 0.437 TA 0.867 | VL 0.642 VA 0.895\n",
      "  ‚úî Save best 0.8953531830073502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 5 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5/30 | TL 0.431 TA 0.858 | VL 0.644 VA 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 6 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E6/30 | TL 0.399 TA 0.898 | VL 0.648 VA 0.896\n",
      "  ‚úî Save best 0.8959760807275445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 7 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E7/30 | TL 0.390 TA 0.884 | VL 0.644 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 8 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E8/30 | TL 0.353 TA 0.918 | VL 0.643 VA 0.895\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import swanlab\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "swanlab.init(project=\"mushroom-toxicity-detection\", run=\"se_cbam_resnet50_mushroom1\")\n",
    "\n",
    "# ---------------------------- Âü∫Á°ÄÈÖçÁΩÆ ----------------------------\n",
    "PROJECT = \"mushroom-toxicity-detection\"\n",
    "RUN_NAME = \"se_cbam_resnet50_v100\"\n",
    "DATA_DIR = Path(\"/workspace/mushroom_dataset_single_split\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 224\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 8\n",
    "MIXUP_ALPHA = 0.32\n",
    "MIXUP_PROB = 0.22\n",
    "TTA_SCALES = [224, 256]\n",
    "LABEL_SMOOTHING = 0.02\n",
    "\n",
    "# ---------------------------- SE ResNet50 + Dropout ----------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    def __init__(self, bottleneck):\n",
    "        super().__init__()\n",
    "        self.body = bottleneck\n",
    "        self.se = SEBlock(bottleneck.conv3.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.se(self.body(x))\n",
    "\n",
    "def build_backbone(num_classes):\n",
    "    m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    for name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        setattr(m, name, nn.Sequential(*[SEBottleneck(b) for b in getattr(m, name)]))\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(2048, num_classes)\n",
    "    )\n",
    "    # ÂÜªÁªìÂâç‰∏§Â±Ç\n",
    "    for param in m.layer1.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in m.layer2.parameters():\n",
    "        param.requires_grad = True\n",
    "    return m\n",
    "\n",
    "# ---------------------------- Êï∞ÊçÆÂ¢ûÂº∫ ----------------------------\n",
    "train_tf = A.Compose([\n",
    "    A.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.HueSaturationValue(10, 15, 10, p=0.5),\n",
    "    A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
    "    A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.05),\n",
    "    A.GaussianBlur(5, p=0.4),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# ---------------------------- Dataset ----------------------------\n",
    "class AlbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, tf):\n",
    "        self.ds = datasets.ImageFolder(root)\n",
    "        self.tf = tf\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        p, l = self.ds.samples[idx]\n",
    "        img = np.asarray(Image.open(p).convert(\"RGB\"))\n",
    "        return self.tf(image=img)[\"image\"], l\n",
    "\n",
    "train_ds = AlbDataset(DATA_DIR/\"train\", train_tf)\n",
    "val_ds = AlbDataset(DATA_DIR/\"val\", val_tf)\n",
    "num_classes = len(train_ds.ds.classes)\n",
    "\n",
    "# -------- WeightedRandomSampler --------\n",
    "labels = [l for _, l in train_ds.ds.samples]\n",
    "counts = np.bincount(labels)\n",
    "weights = 1.0 / counts\n",
    "sample_weights = [weights[l] for l in labels]\n",
    "train_sampler = WeightedRandomSampler(sample_weights, len(train_ds), replacement=True)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                    num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------- MixUp ----------------------------\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ---------------------------- Label Smoothing Loss ----------------------------\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logprobs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * logprobs, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingLoss(num_classes, smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# ---------------------------- ‰ºòÂåñÂô® & Ë∞ÉÂ∫¶ ----------------------------\n",
    "model = build_backbone(num_classes).to(DEVICE)\n",
    "param_groups = [\n",
    "    {\"params\": model.layer1.parameters(), \"lr\": 1e-6},\n",
    "    {\"params\": model.layer2.parameters(), \"lr\": 5e-6},\n",
    "    {\"params\": model.layer3.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.layer4.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 1e-4},\n",
    "]\n",
    "\n",
    "opt = optim.AdamW(param_groups, weight_decay=1e-4)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler = GradScaler()\n",
    "\n",
    "class EarlyStop:\n",
    "    def __init__(self, p=7, delta=0.001):\n",
    "        self.p, self.d = p, delta; self.best = 1e9; self.c = 0\n",
    "    def __call__(self, v):\n",
    "        if v < self.best - self.d: self.best, self.c = v, 0\n",
    "        else: self.c += 1\n",
    "        return self.c >= self.p\n",
    "\n",
    "estop = EarlyStop()\n",
    "\n",
    "# Âä†ËΩΩ‰πãÂâç‰øùÂ≠òÁöÑÊúÄ‰ºòÊ®°Âûã\n",
    "best_model_path = \"/workspace/best_se_resnet50_mushroom11.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    \n",
    "# ---------------------------- ËÆ≠ÁªÉÂæ™ÁéØ ----------------------------\n",
    "def train():\n",
    "    best = 0\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tl, tc = 0, 0\n",
    "        train_pbar = tqdm(train_ld, desc=f\"Epoch {ep} Train\", unit=\"it\", dynamic_ncols=True)\n",
    "        for xb, yb in train_pbar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            lam = 1.0\n",
    "            if random.random() < MIXUP_PROB:\n",
    "                xb, y_a, y_b, lam = mixup_data(xb, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                logits = model(xb)\n",
    "                if lam == 1.0:\n",
    "                    loss = criterion(logits, yb)\n",
    "                else:\n",
    "                    loss = lam*criterion(logits, y_a) + (1-lam)*criterion(logits, y_b)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            tl += loss.item()*xb.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            tc += (preds==yb).sum().item()\n",
    "        train_loss = tl/len(train_ds); train_acc = tc/len(train_ds)\n",
    "\n",
    "        model.eval(); vl, vc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_ld, desc=f\"Epoch {ep} Val\", unit=\"it\", dynamic_ncols=True)\n",
    "            for xb, yb in val_pbar:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                vl += loss.item()*xb.size(0)\n",
    "\n",
    "\n",
    "                vc += (logits.argmax(1)== yb).sum().item()\n",
    "        val_loss = vl/len(val_ds); val_acc = vc/len(val_ds)\n",
    "        sched.step()\n",
    "\n",
    "        print(f\"E{ep}/{EPOCHS} | TL {train_loss:.3f} TA {train_acc:.3f} | VL {val_loss:.3f} VA {val_acc:.3f}\")\n",
    "        swanlab.log({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc,\"lr\":sched.get_last_lr()[0]})\n",
    "\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "            torch.save(model.state_dict(), \"/workspace/best_se_resnet50_mushroom12.pth\")\n",
    "            print(\"  ‚úî Save best\", best)\n",
    "        if estop(val_loss):\n",
    "            print(\"Early stop!\"); break\n",
    "    torch.save(model.state_dict(), \"/workspace/last_se_resnet50_mushroom12.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: \\ Waiting for the swanlab cloud response."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: swanlab version 0.6.5 is available!  Upgrade: `pip install -U swanlab`    \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.1                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/swanlog/run-20250707_174719-fd2802a2\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üëã Hi \u001b[1m\u001b[39mSZY_230507\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mdog-12\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üè† View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/zk95qa07y3kfq4vrtkq47\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/zk95qa07y3kfq4vrtkq47\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2326/1845546405.py:86: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.05),\n",
      "/tmp/ipykernel_2326/1845546405.py:176: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1 Train:   0%|          | 0/290 [00:00<?, ?it/s]/tmp/ipykernel_2326/1845546405.py:206: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 1 Val:   0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_2326/1845546405.py:223: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:08<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1/30 | TL 0.379 TA 0.895 | VL 0.638 VA 0.894\n",
      "  ‚úî Save best 0.8937336489348449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 2 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2/30 | TL 0.401 TA 0.892 | VL 0.637 VA 0.895\n",
      "  ‚úî Save best 0.8947302852871558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 3 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E3/30 | TL 0.385 TA 0.893 | VL 0.649 VA 0.895\n",
      "  ‚úî Save best 0.8952286034633113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 4 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4/30 | TL 0.384 TA 0.889 | VL 0.647 VA 0.895\n",
      "  ‚úî Save best 0.8954777625513891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 5 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5/30 | TL 0.444 TA 0.869 | VL 0.651 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 6 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E6/30 | TL 0.436 TA 0.870 | VL 0.639 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 7 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E7/30 | TL 0.386 TA 0.886 | VL 0.639 VA 0.896\n",
      "  ‚úî Save best 0.8956023420954279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 8 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E8/30 | TL 0.379 TA 0.894 | VL 0.647 VA 0.896\n",
      "  ‚úî Save best 0.8962252398156223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 9 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E9/30 | TL 0.399 TA 0.894 | VL 0.641 VA 0.898\n",
      "  ‚úî Save best 0.8977201943440887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 10 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:08<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E10/30 | TL 0.363 TA 0.890 | VL 0.632 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 11 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E11/30 | TL 0.407 TA 0.861 | VL 0.644 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 12 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E12/30 | TL 0.403 TA 0.883 | VL 0.644 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 13 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E13/30 | TL 0.446 TA 0.872 | VL 0.641 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 14 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E14/30 | TL 0.381 TA 0.857 | VL 0.636 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 15 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ SWA started from epoch 15\n",
      "E15/30 | TL 0.390 TA 0.864 | VL 0.632 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 16 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E16/30 | TL 0.408 TA 0.898 | VL 0.630 VA 0.899\n",
      "  ‚úî Save best 0.8988414102404385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 17 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E17/30 | TL 0.408 TA 0.895 | VL 0.636 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 18 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E18/30 | TL 0.386 TA 0.916 | VL 0.640 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 19 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E19/30 | TL 0.421 TA 0.839 | VL 0.632 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 20 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E20/30 | TL 0.423 TA 0.882 | VL 0.629 VA 0.899\n",
      "  ‚úî Save best 0.8992151488725552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 21 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E21/30 | TL 0.397 TA 0.878 | VL 0.635 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 22 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E22/30 | TL 0.402 TA 0.900 | VL 0.633 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 23 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E23/30 | TL 0.394 TA 0.879 | VL 0.637 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 24 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E24/30 | TL 0.398 TA 0.888 | VL 0.644 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 25 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E25/30 | TL 0.335 TA 0.905 | VL 0.659 VA 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 26 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E26/30 | TL 0.407 TA 0.893 | VL 0.646 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 27 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E27/30 | TL 0.415 TA 0.861 | VL 0.637 VA 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 28 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E28/30 | TL 0.358 TA 0.886 | VL 0.650 VA 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 29 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E29/30 | TL 0.433 TA 0.885 | VL 0.632 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 30 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E30/30 | TL 0.435 TA 0.892 | VL 0.632 VA 0.895\n",
      "üì¶ SWA model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import swanlab\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import ImageFile\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "swanlab.init(project=\"mushroom-toxicity-detection\", run=\"se_cbam_resnet50_mushroom1\")\n",
    "\n",
    "# ---------------------------- Âü∫Á°ÄÈÖçÁΩÆ ----------------------------\n",
    "PROJECT = \"mushroom-toxicity-detection\"\n",
    "RUN_NAME = \"se_cbam_resnet50_v100\"\n",
    "DATA_DIR = Path(\"/workspace/mushroom_dataset_single_split\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 224\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 8\n",
    "MIXUP_ALPHA = 0.32\n",
    "MIXUP_PROB = 0.22\n",
    "TTA_SCALES = [224, 256]\n",
    "LABEL_SMOOTHING = 0.02\n",
    "\n",
    "# ---------------------------- SE ResNet50 + Dropout ----------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    def __init__(self, bottleneck):\n",
    "        super().__init__()\n",
    "        self.body = bottleneck\n",
    "        self.se = SEBlock(bottleneck.conv3.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.se(self.body(x))\n",
    "\n",
    "def build_backbone(num_classes):\n",
    "    m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    for name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        setattr(m, name, nn.Sequential(*[SEBottleneck(b) for b in getattr(m, name)]))\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(2048, num_classes)\n",
    "    )\n",
    "    # ÂÜªÁªìÂâç‰∏§Â±Ç\n",
    "    for param in m.layer1.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in m.layer2.parameters():\n",
    "        param.requires_grad = True\n",
    "    return m\n",
    "\n",
    "# ---------------------------- Êï∞ÊçÆÂ¢ûÂº∫ ----------------------------\n",
    "train_tf = A.Compose([\n",
    "    A.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.HueSaturationValue(10, 15, 10, p=0.5),\n",
    "    A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
    "    A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.05),\n",
    "    A.GaussianBlur(5, p=0.4),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# ---------------------------- Dataset ----------------------------\n",
    "class AlbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, tf):\n",
    "        self.ds = datasets.ImageFolder(root)\n",
    "        self.tf = tf\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        p, l = self.ds.samples[idx]\n",
    "        img = np.asarray(Image.open(p).convert(\"RGB\"))\n",
    "        return self.tf(image=img)[\"image\"], l\n",
    "\n",
    "train_ds = AlbDataset(DATA_DIR/\"train\", train_tf)\n",
    "val_ds = AlbDataset(DATA_DIR/\"val\", val_tf)\n",
    "num_classes = len(train_ds.ds.classes)\n",
    "\n",
    "# -------- WeightedRandomSampler --------\n",
    "labels = [l for _, l in train_ds.ds.samples]\n",
    "counts = np.bincount(labels)\n",
    "weights = 1.0 / counts\n",
    "sample_weights = [weights[l] for l in labels]\n",
    "train_sampler = WeightedRandomSampler(sample_weights, len(train_ds), replacement=True)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                    num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------- MixUp ----------------------------\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ---------------------------- Label Smoothing Loss ----------------------------\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logprobs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * logprobs, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingLoss(num_classes, smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# ---------------------------- ‰ºòÂåñÂô® & Ë∞ÉÂ∫¶ ----------------------------\n",
    "model = build_backbone(num_classes).to(DEVICE)\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": model.layer1.parameters(), \"lr\": 1e-6},\n",
    "    {\"params\": model.layer2.parameters(), \"lr\": 5e-6},\n",
    "    {\"params\": model.layer3.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.layer4.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 1e-4},\n",
    "]\n",
    "\n",
    "\n",
    "opt = optim.AdamW(param_groups, weight_decay=1e-4)\n",
    "# SWA ËÆæÁΩÆ\n",
    "swa_start = 15  # Á¨¨Âá†ËΩÆÂºÄÂßãSWAÔºåÂèØË∞É\n",
    "swa_model = AveragedModel(model)\n",
    "swa_scheduler = SWALR(opt, swa_lr=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler = GradScaler()\n",
    "\n",
    "class EarlyStop:\n",
    "    def __init__(self, p=7, delta=0.001):\n",
    "        self.p, self.d = p, delta; self.best = 1e9; self.c = 0\n",
    "    def __call__(self, v):\n",
    "        if v < self.best - self.d: self.best, self.c = v, 0\n",
    "        else: self.c += 1\n",
    "        return self.c >= self.p\n",
    "\n",
    "estop = EarlyStop(p=99)\n",
    "\n",
    "# Âä†ËΩΩ‰πãÂâç‰øùÂ≠òÁöÑÊúÄ‰ºòÊ®°Âûã\n",
    "best_model_path = \"/workspace/best_se_resnet50_mushroom12.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    \n",
    "# ---------------------------- ËÆ≠ÁªÉÂæ™ÁéØ ----------------------------\n",
    "def train():\n",
    "    best = 0\n",
    "    use_swa = False  # ÂºÄÂÖ≥ÔºåÂú®Á¨¨ swa_start ËΩÆÂºÄÂêØ\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tl, tc = 0, 0\n",
    "        train_pbar = tqdm(train_ld, desc=f\"Epoch {ep} Train\", unit=\"it\", dynamic_ncols=True)\n",
    "        for xb, yb in train_pbar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            lam = 1.0\n",
    "            if random.random() < MIXUP_PROB:\n",
    "                xb, y_a, y_b, lam = mixup_data(xb, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                logits = model(xb)\n",
    "                if lam == 1.0:\n",
    "                    loss = criterion(logits, yb)\n",
    "                else:\n",
    "                    loss = lam*criterion(logits, y_a) + (1-lam)*criterion(logits, y_b)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            tl += loss.item()*xb.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            tc += (preds==yb).sum().item()\n",
    "        train_loss = tl/len(train_ds); train_acc = tc/len(train_ds)\n",
    "\n",
    "        model.eval(); vl, vc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_ld, desc=f\"Epoch {ep} Val\", unit=\"it\", dynamic_ncols=True)\n",
    "            for xb, yb in val_pbar:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                vl += loss.item()*xb.size(0)\n",
    "\n",
    "\n",
    "                vc += (logits.argmax(1)== yb).sum().item()\n",
    "        val_loss = vl/len(val_ds); val_acc = vc/len(val_ds)\n",
    "        # SWA Êõ¥Êñ∞\n",
    "        if ep >= swa_start:\n",
    "            if not use_swa:\n",
    "                print(f\"üîÅ SWA started from epoch {ep}\")\n",
    "                use_swa = True\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        else:\n",
    "            sched.step()\n",
    "\n",
    "        print(f\"E{ep}/{EPOCHS} | TL {train_loss:.3f} TA {train_acc:.3f} | VL {val_loss:.3f} VA {val_acc:.3f}\")\n",
    "        swanlab.log({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc,\"lr\":sched.get_last_lr()[0]})\n",
    "\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "            if use_swa:\n",
    "                torch.save(swa_model.module.state_dict(), \"/workspace/best_swa_model.pth\")\n",
    "            else:\n",
    "                torch.save(model.state_dict(), \"/workspace/best_se_resnet50_mushroom13.pth\")\n",
    "            print(\"  ‚úî Save best\", best)\n",
    "        if estop(val_loss):\n",
    "            print(\"Early stop!\"); break\n",
    "    if use_swa:\n",
    "        torch.optim.swa_utils.update_bn(train_ld, swa_model, device=DEVICE)\n",
    "        torch.save(swa_model.module.state_dict(), \"/workspace/best_swa_model.pth\")\n",
    "        print(\"üì¶ SWA model saved.\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \"/workspace/last_se_resnet50_mushroom13.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: swanlab version 0.6.5 is available!  Upgrade: `pip install -U swanlab`    \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.1                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/swanlog/run-20250708_043714-5f256344\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üëã Hi \u001b[1m\u001b[39mSZY_230507\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mdog-12\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üè† View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/1seep8rhaz5uuyj3b1zpf\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/1seep8rhaz5uuyj3b1zpf\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2230/3972247706.py:86: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.05),\n",
      "/tmp/ipykernel_2230/3972247706.py:176: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1 Train:   0%|          | 0/290 [00:00<?, ?it/s]/tmp/ipykernel_2230/3972247706.py:206: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:55<00:00,  2.51it/s]\n",
      "Epoch 1 Val:   0%|          | 0/36 [00:00<?, ?it/s]/tmp/ipykernel_2230/3972247706.py:223: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:09<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1/50 | TL 0.579 TA 0.922 | VL 0.853 VA 0.897\n",
      "  ‚úî Save best 0.8968481375358166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 2 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2/50 | TL 0.592 TA 0.873 | VL 0.861 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 3 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E3/50 | TL 0.583 TA 0.872 | VL 0.862 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 4 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4/50 | TL 0.592 TA 0.887 | VL 0.860 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 5 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5/50 | TL 0.587 TA 0.900 | VL 0.865 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 6 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E6/50 | TL 0.605 TA 0.875 | VL 0.868 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 7 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E7/50 | TL 0.611 TA 0.882 | VL 0.860 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 8 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E8/50 | TL 0.572 TA 0.891 | VL 0.866 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 9 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E9/50 | TL 0.577 TA 0.916 | VL 0.860 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 10 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E10/50 | TL 0.589 TA 0.886 | VL 0.860 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 11 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E11/50 | TL 0.595 TA 0.878 | VL 0.857 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 12 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E12/50 | TL 0.607 TA 0.879 | VL 0.861 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 13 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:08<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E13/50 | TL 0.626 TA 0.874 | VL 0.865 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 14 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E14/50 | TL 0.620 TA 0.873 | VL 0.858 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 15 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E15/50 | TL 0.569 TA 0.888 | VL 0.863 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 16 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:08<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E16/50 | TL 0.605 TA 0.883 | VL 0.858 VA 0.897\n",
      "  ‚úî Save best 0.8970972966238944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 17 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E17/50 | TL 0.588 TA 0.887 | VL 0.872 VA 0.898\n",
      "  ‚úî Save best 0.8979693534321664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 18 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E18/50 | TL 0.608 TA 0.900 | VL 0.864 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 19 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E19/50 | TL 0.539 TA 0.896 | VL 0.870 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 20 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E20/50 | TL 0.562 TA 0.914 | VL 0.866 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 21 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E21/50 | TL 0.598 TA 0.885 | VL 0.861 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 22 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E22/50 | TL 0.589 TA 0.914 | VL 0.869 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 23 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E23/50 | TL 0.597 TA 0.877 | VL 0.870 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 24 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E24/50 | TL 0.560 TA 0.879 | VL 0.866 VA 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 25 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E25/50 | TL 0.589 TA 0.873 | VL 0.861 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 26 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E26/50 | TL 0.625 TA 0.865 | VL 0.867 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 27 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E27/50 | TL 0.638 TA 0.882 | VL 0.861 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 28 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E28/50 | TL 0.624 TA 0.881 | VL 0.864 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 29 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E29/50 | TL 0.620 TA 0.868 | VL 0.864 VA 0.899\n",
      "  ‚úî Save best 0.8985922511523607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 30 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ SWA started from epoch 30\n",
      "E30/50 | TL 0.595 TA 0.880 | VL 0.862 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 31 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E31/50 | TL 0.579 TA 0.903 | VL 0.865 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 32 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E32/50 | TL 0.575 TA 0.893 | VL 0.856 VA 0.899\n",
      "  ‚úî Save best 0.899339728416594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 33 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E33/50 | TL 0.615 TA 0.879 | VL 0.866 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 34 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E34/50 | TL 0.547 TA 0.915 | VL 0.868 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 35 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E35/50 | TL 0.636 TA 0.883 | VL 0.872 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 36 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E36/50 | TL 0.568 TA 0.894 | VL 0.873 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 37 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E37/50 | TL 0.599 TA 0.888 | VL 0.871 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 38 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E38/50 | TL 0.581 TA 0.895 | VL 0.879 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 39 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E39/50 | TL 0.560 TA 0.903 | VL 0.869 VA 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 40 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E40/50 | TL 0.615 TA 0.886 | VL 0.863 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 41 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E41/50 | TL 0.568 TA 0.905 | VL 0.875 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 42 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E42/50 | TL 0.616 TA 0.856 | VL 0.870 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 43 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E43/50 | TL 0.574 TA 0.899 | VL 0.870 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 44 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E44/50 | TL 0.544 TA 0.900 | VL 0.874 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.76it/s]\n",
      "Epoch 45 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:08<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E45/50 | TL 0.582 TA 0.907 | VL 0.868 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 46 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E46/50 | TL 0.563 TA 0.913 | VL 0.865 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 47 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E47/50 | TL 0.573 TA 0.900 | VL 0.876 VA 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 48 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:08<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E48/50 | TL 0.580 TA 0.899 | VL 0.869 VA 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.75it/s]\n",
      "Epoch 49 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:07<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E49/50 | TL 0.560 TA 0.895 | VL 0.880 VA 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [01:45<00:00,  2.74it/s]\n",
      "Epoch 50 Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:08<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E50/50 | TL 0.574 TA 0.873 | VL 0.877 VA 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ SWA model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import swanlab\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import ImageFile\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "swanlab.init(project=\"mushroom-toxicity-detection\", run=\"se_cbam_resnet50_mushroom1\")\n",
    "\n",
    "# ---------------------------- Âü∫Á°ÄÈÖçÁΩÆ ----------------------------\n",
    "PROJECT = \"mushroom-toxicity-detection\"\n",
    "RUN_NAME = \"se_cbam_resnet50_v100\"\n",
    "DATA_DIR = Path(\"/workspace/mushroom_dataset_single_split\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 224\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 8\n",
    "MIXUP_ALPHA = 0.15\n",
    "MIXUP_PROB = 0.15\n",
    "TTA_SCALES = [224, 256]\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "# ---------------------------- SE ResNet50 + Dropout ----------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        w = self.pool(x).view(b, c)\n",
    "        w = self.fc(w).view(b, c, 1, 1)\n",
    "        return x * w\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    def __init__(self, bottleneck):\n",
    "        super().__init__()\n",
    "        self.body = bottleneck\n",
    "        self.se = SEBlock(bottleneck.conv3.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.se(self.body(x))\n",
    "\n",
    "def build_backbone(num_classes):\n",
    "    m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    for name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        setattr(m, name, nn.Sequential(*[SEBottleneck(b) for b in getattr(m, name)]))\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(2048, num_classes)\n",
    "    )\n",
    "    \n",
    "    for param in m.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return m\n",
    "\n",
    "# ---------------------------- Êï∞ÊçÆÂ¢ûÂº∫ ----------------------------\n",
    "train_tf = A.Compose([\n",
    "    A.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.HueSaturationValue(10, 15, 10, p=0.5),\n",
    "    A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
    "    A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.05),\n",
    "    A.GaussianBlur(5, p=0.4),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# ---------------------------- Dataset ----------------------------\n",
    "class AlbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, tf):\n",
    "        self.ds = datasets.ImageFolder(root)\n",
    "        self.tf = tf\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        p, l = self.ds.samples[idx]\n",
    "        img = np.asarray(Image.open(p).convert(\"RGB\"))\n",
    "        return self.tf(image=img)[\"image\"], l\n",
    "\n",
    "train_ds = AlbDataset(DATA_DIR/\"train\", train_tf)\n",
    "val_ds = AlbDataset(DATA_DIR/\"val\", val_tf)\n",
    "num_classes = len(train_ds.ds.classes)\n",
    "\n",
    "# -------- WeightedRandomSampler --------\n",
    "labels = [l for _, l in train_ds.ds.samples]\n",
    "counts = np.bincount(labels)\n",
    "weights = 1.0 / counts\n",
    "sample_weights = [weights[l] for l in labels]\n",
    "train_sampler = WeightedRandomSampler(sample_weights, len(train_ds), replacement=True)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                    num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------- MixUp ----------------------------\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ---------------------------- Label Smoothing Loss ----------------------------\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logprobs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * logprobs, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingLoss(num_classes, smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# ---------------------------- ‰ºòÂåñÂô® & Ë∞ÉÂ∫¶ ----------------------------\n",
    "model = build_backbone(num_classes).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "opt = optim.AdamW(param_groups, weight_decay=1e-4)\n",
    "# SWA ËÆæÁΩÆ\n",
    "swa_start = 12  # Á¨¨Âá†ËΩÆÂºÄÂßãSWAÔºåÂèØË∞É\n",
    "swa_model = AveragedModel(model)\n",
    "swa_scheduler = SWALR(opt, swa_lr=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler = GradScaler()\n",
    "\n",
    "class EarlyStop:\n",
    "    def __init__(self, p=7, delta=0.001):\n",
    "        self.p, self.d = p, delta; self.best = 1e9; self.c = 0\n",
    "    def __call__(self, v):\n",
    "        if v < self.best - self.d: self.best, self.c = v, 0\n",
    "        else: self.c += 1\n",
    "        return self.c >= self.p\n",
    "\n",
    "estop = EarlyStop(p=99)\n",
    "\n",
    "# Âä†ËΩΩ‰πãÂâç‰øùÂ≠òÁöÑÊúÄ‰ºòÊ®°Âûã\n",
    "best_model_path = \"/workspace/best_se_resnet50_mushroom14.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    \n",
    "# ---------------------------- ËÆ≠ÁªÉÂæ™ÁéØ ----------------------------\n",
    "def train():\n",
    "    best = 0\n",
    "    use_swa = False  # ÂºÄÂÖ≥ÔºåÂú®Á¨¨ swa_start ËΩÆÂºÄÂêØ\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tl, tc = 0, 0\n",
    "        train_pbar = tqdm(train_ld, desc=f\"Epoch {ep} Train\", unit=\"it\", dynamic_ncols=True)\n",
    "        for xb, yb in train_pbar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            lam = 1.0\n",
    "            if random.random() < MIXUP_PROB:\n",
    "                xb, y_a, y_b, lam = mixup_data(xb, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                logits = model(xb)\n",
    "                if lam == 1.0:\n",
    "                    loss = criterion(logits, yb)\n",
    "                else:\n",
    "                    loss = lam*criterion(logits, y_a) + (1-lam)*criterion(logits, y_b)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            tl += loss.item()*xb.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            tc += (preds==yb).sum().item()\n",
    "        train_loss = tl/len(train_ds); train_acc = tc/len(train_ds)\n",
    "\n",
    "        model.eval(); vl, vc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_ld, desc=f\"Epoch {ep} Val\", unit=\"it\", dynamic_ncols=True)\n",
    "            for xb, yb in val_pbar:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                vl += loss.item()*xb.size(0)\n",
    "\n",
    "\n",
    "                vc += (logits.argmax(1)== yb).sum().item()\n",
    "        val_loss = vl/len(val_ds); val_acc = vc/len(val_ds)\n",
    "        # SWA Êõ¥Êñ∞\n",
    "        if ep >= swa_start:\n",
    "            if not use_swa:\n",
    "                print(f\"üîÅ SWA started from epoch {ep}\")\n",
    "                use_swa = True\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        else:\n",
    "            sched.step()\n",
    "\n",
    "        print(f\"E{ep}/{EPOCHS} | TL {train_loss:.3f} TA {train_acc:.3f} | VL {val_loss:.3f} VA {val_acc:.3f}\")\n",
    "        swanlab.log({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc,\"lr\":sched.get_last_lr()[0]})\n",
    "\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "            if use_swa:\n",
    "                torch.save(swa_model.module.state_dict(), \"/workspace/best_swa_model.pth\")\n",
    "            else:\n",
    "                torch.save(model.state_dict(), \"/workspace/best_se_resnet50_mushroom15.pth\")\n",
    "            print(\"  ‚úî Save best\", best)\n",
    "        if estop(val_loss):\n",
    "            print(\"Early stop!\"); break\n",
    "    if use_swa:\n",
    "        torch.optim.swa_utils.update_bn(train_ld, swa_model, device=DEVICE)\n",
    "        torch.save(swa_model.module.state_dict(), \"/workspace/best_swa_model.pth\")\n",
    "        print(\"üì¶ SWA model saved.\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \"/workspace/last_se_resnet50_mushroom15.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üè† View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@SZY_230507/mushroom-toxicity-detection/runs/99ipcz7n12jh9pjv87wwd\u001b[0m\u001b[0m\n",
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "swanlab.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: \\ Waiting for the swanlab cloud response."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: swanlab version 0.6.5 is available!  Upgrade: `pip install -U swanlab`    \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: \\ Creating experiment...                                                  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mswa_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AveragedModel, SWALR\n\u001b[32m     20\u001b[39m ImageFile.LOAD_TRUNCATED_IMAGES = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mswanlab\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmushroom-toxicity-detection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mse_cbam_resnet50_mushroom1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ---------------------------- Âü∫Á°ÄÈÖçÁΩÆ ----------------------------\u001b[39;00m\n\u001b[32m     25\u001b[39m PROJECT = \u001b[33m\"\u001b[39m\u001b[33mmushroom-toxicity-detection\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swanlab/data/sdk.py:206\u001b[39m, in \u001b[36mSwanLabInitializer.init\u001b[39m\u001b[34m(self, project, workspace, experiment_name, description, tags, config, logdir, mode, load, public, callbacks, settings, **kwargs)\u001b[39m\n\u001b[32m    203\u001b[39m config = _init_config(config)\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# ---------------------------------- ÂÆû‰æãÂåñÂÆûÈ™å ----------------------------------\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Ê≥®ÂÜåÂÆûÈ™å\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m run = \u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog_level\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minfo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_num\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m run\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swanlab/data/run/__init__.py:15\u001b[39m, in \u001b[36mregister\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mregister\u001b[39m(*args, **kwargs) -> SwanLabRun:\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Ê≥®ÂÜåÂπ∂ÂÆû‰æãÂåñSwanLabRunÁ±ª\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     run = \u001b[43mSwanLabRun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swanlab/data/run/main.py:125\u001b[39m, in \u001b[36mSwanLabRun.__init__\u001b[39m\u001b[34m(self, project_name, experiment_name, description, tags, run_config, log_level, exp_num, operator)\u001b[39m\n\u001b[32m    122\u001b[39m run = \u001b[38;5;28mself\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# ---------------------------------- ÂàùÂßãÂåñÂÆåÊàê ----------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__operator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# ÊâßË°å__saveÔºåÂøÖÈ°ªÂú®on_run‰πãÂêéÔºåÂõ†‰∏∫on_run‰πãÂâçÈÉ®ÂàÜÁöÑ‰ø°ÊÅØËøòÊ≤°ÂÆåÂÖ®ÂàùÂßãÂåñ\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_SwanLabConfig__save\u001b[39m\u001b[33m\"\u001b[39m)()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swanlab/data/run/helper.py:109\u001b[39m, in \u001b[36mSwanLabRunOperator.on_run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_run\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__run_all\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_run\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     try_send_webhook()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swanlab/data/run/helper.py:59\u001b[39m, in \u001b[36mSwanLabRunOperator.__run_all\u001b[39m\u001b[34m(self, method, *args, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__run_all\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swanlab/data/run/helper.py:59\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__run_all\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swanlab/data/callbacker/cloud.py:132\u001b[39m, in \u001b[36mCloudRunCallback.on_run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m http = get_http()\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Ê≥®ÂÜåÂÆûÈ™å‰ø°ÊÅØ\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmount_exp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp_colors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# Ê≥®ÂÜåÁªàÁ´ØËæìÂá∫ÊµÅ‰ª£ÁêÜ\u001b[39;00m\n\u001b[32m    139\u001b[39m settings = get_settings()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swanlab/api/http.py:329\u001b[39m, in \u001b[36mHTTP.mount_exp\u001b[39m\u001b[34m(self, exp_name, colors, description, tags)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;66;03m# Ëé∑Âèñcos‰ø°ÊÅØ\u001b[39;00m\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mself\u001b[39m.__get_cos()\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m \u001b[43mFONT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloading\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCreating experiment...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/swankit/log/utils.py:82\u001b[39m, in \u001b[36mFONT.loading\u001b[39m\u001b[34m(s, func, args, interval, prefix, brush_length)\u001b[39m\n\u001b[32m     80\u001b[39m     running = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43mt1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/threading.py:1112\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1114\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1115\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.1/lib/python3.11/threading.py:1132\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1133\u001b[39m         lock.release()\n\u001b[32m   1134\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import swanlab\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import ImageFile\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "swanlab.init(project=\"mushroom-toxicity-detection2\", run=\"se_cbam_resnet50_mushroom1\")\n",
    "\n",
    "# ---------------------------- Âü∫Á°ÄÈÖçÁΩÆ ----------------------------\n",
    "PROJECT = \"mushroom-toxicity-detection\"\n",
    "RUN_NAME = \"se_cbam_resnet50_v100\"\n",
    "DATA_DIR = Path(\"/workspace/mushroom_dataset_single_split\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "NUM_WORKERS = 8\n",
    "MIXUP_ALPHA = 0.15\n",
    "MIXUP_PROB = 0.15\n",
    "TTA_SCALES = [224, 256]\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- CBAM Ê®°Âùó ----------------------------\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_planes, in_planes // ratio, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_planes // ratio, in_planes, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        out = avg_out + max_out\n",
    "        return x * self.sigmoid(out).view(b, c, 1, 1)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return x * self.sigmoid(self.conv(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ca(x)\n",
    "        x = self.sa(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------- ‰øÆÊîπ SEBottleneck Âä† CBAM ----------------------------\n",
    "class SECBAMBottleneck(nn.Module):\n",
    "    def __init__(self, bottleneck):\n",
    "        super().__init__()\n",
    "        self.body = bottleneck\n",
    "        out_channels = bottleneck.conv3.out_channels\n",
    "        self.se = SEBlock(out_channels)\n",
    "        self.cbam = CBAM(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.se(x)\n",
    "        x = self.cbam(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------- ÊõøÊç¢ÂéüÂßãÊûÑÂª∫ÂáΩÊï∞ ----------------------------\n",
    "def build_backbone(num_classes):\n",
    "    m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    for name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        blocks = [SECBAMBottleneck(b) for b in getattr(m, name)]\n",
    "        setattr(m, name, nn.Sequential(*blocks))\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(2048, num_classes)\n",
    "    )\n",
    "    for param in m.parameters():\n",
    "        param.requires_grad = True\n",
    "    return m\n",
    "\n",
    "\n",
    "# ---------------------------- Êï∞ÊçÆÂ¢ûÂº∫ ----------------------------\n",
    "train_tf = A.Compose([\n",
    "    A.RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0), ratio=(0.75, 1.33)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.HueSaturationValue(10, 15, 10, p=0.5),\n",
    "    A.RandomBrightnessContrast(0.2, 0.2, p=0.5),\n",
    "    A.CoarseDropout(max_holes=4, max_height=16, max_width=16, p=0.05),\n",
    "    A.GaussianBlur(5, p=0.4),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "val_tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# ---------------------------- Dataset ----------------------------\n",
    "class AlbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, tf):\n",
    "        self.ds = datasets.ImageFolder(root)\n",
    "        self.tf = tf\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        p, l = self.ds.samples[idx]\n",
    "        img = np.asarray(Image.open(p).convert(\"RGB\"))\n",
    "        return self.tf(image=img)[\"image\"], l\n",
    "\n",
    "train_ds = AlbDataset(DATA_DIR/\"train\", train_tf)\n",
    "val_ds = AlbDataset(DATA_DIR/\"val\", val_tf)\n",
    "num_classes = len(train_ds.ds.classes)\n",
    "\n",
    "# -------- WeightedRandomSampler --------\n",
    "labels = [l for _, l in train_ds.ds.samples]\n",
    "counts = np.bincount(labels)\n",
    "weights = 1.0 / counts\n",
    "sample_weights = [weights[l] for l in labels]\n",
    "train_sampler = WeightedRandomSampler(sample_weights, len(train_ds), replacement=True)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE,shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                    num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# ---------------------------- MixUp ----------------------------\n",
    "def mixup_data(x, y, alpha=MIXUP_ALPHA):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ---------------------------- Label Smoothing Loss ----------------------------\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(logprobs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * logprobs, dim=-1))\n",
    "\n",
    "criterion = LabelSmoothingLoss(num_classes, smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# ---------------------------- ‰ºòÂåñÂô® & Ë∞ÉÂ∫¶ ----------------------------\n",
    "model = build_backbone(num_classes).to(DEVICE)\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": model.conv1.parameters(), \"lr\": 1e-6},\n",
    "    {\"params\": model.bn1.parameters(), \"lr\": 1e-6},\n",
    "    {\"params\": model.layer1.parameters(), \"lr\": 1e-6},\n",
    "    {\"params\": model.layer2.parameters(), \"lr\": 5e-6},\n",
    "    {\"params\": model.layer3.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.layer4.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 1e-4},\n",
    "]\n",
    "\n",
    "\n",
    "opt = optim.AdamW(param_groups, weight_decay=1e-4)\n",
    "# SWA ËÆæÁΩÆ\n",
    "swa_start = 12  # Á¨¨Âá†ËΩÆÂºÄÂßãSWAÔºåÂèØË∞É\n",
    "swa_model = AveragedModel(model)\n",
    "swa_scheduler = SWALR(opt, swa_lr=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler = GradScaler()\n",
    "\n",
    "class EarlyStop:\n",
    "    def __init__(self, p=7, delta=0.001):\n",
    "        self.p, self.d = p, delta; self.best = 1e9; self.c = 0\n",
    "    def __call__(self, v):\n",
    "        if v < self.best - self.d: self.best, self.c = v, 0\n",
    "        else: self.c += 1\n",
    "        return self.c >= self.p\n",
    "\n",
    "estop = EarlyStop(p=99)\n",
    "\n",
    "# Âä†ËΩΩ‰πãÂâç‰øùÂ≠òÁöÑÊúÄ‰ºòÊ®°Âûã\n",
    "best_model_path = \"/workspace/best_se_resnet50_mushroom14.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    \n",
    "# ---------------------------- ËÆ≠ÁªÉÂæ™ÁéØ ----------------------------\n",
    "def train():\n",
    "    best = 0\n",
    "    use_swa = False  # ÂºÄÂÖ≥ÔºåÂú®Á¨¨ swa_start ËΩÆÂºÄÂêØ\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tl, tc = 0, 0\n",
    "        train_pbar = tqdm(train_ld, desc=f\"Epoch {ep} Train\", unit=\"it\", dynamic_ncols=True)\n",
    "        for xb, yb in train_pbar:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            lam = 1.0\n",
    "            if random.random() < MIXUP_PROB:\n",
    "                xb, y_a, y_b, lam = mixup_data(xb, yb)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                logits = model(xb)\n",
    "                if lam == 1.0:\n",
    "                    loss = criterion(logits, yb)\n",
    "                else:\n",
    "                    loss = lam*criterion(logits, y_a) + (1-lam)*criterion(logits, y_b)\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            tl += loss.item()*xb.size(0)\n",
    "            preds = logits.argmax(1)\n",
    "            tc += (preds==yb).sum().item()\n",
    "        train_loss = tl/len(train_ds); train_acc = tc/len(train_ds)\n",
    "\n",
    "        model.eval(); vl, vc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_ld, desc=f\"Epoch {ep} Val\", unit=\"it\", dynamic_ncols=True)\n",
    "            for xb, yb in val_pbar:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                with autocast():\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                vl += loss.item()*xb.size(0)\n",
    "\n",
    "\n",
    "                vc += (logits.argmax(1)== yb).sum().item()\n",
    "        val_loss = vl/len(val_ds); val_acc = vc/len(val_ds)\n",
    "        # SWA Êõ¥Êñ∞\n",
    "        if ep >= swa_start:\n",
    "            if not use_swa:\n",
    "                print(f\"üîÅ SWA started from epoch {ep}\")\n",
    "                use_swa = True\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        else:\n",
    "            sched.step()\n",
    "\n",
    "        print(f\"E{ep}/{EPOCHS} | TL {train_loss:.3f} TA {train_acc:.3f} | VL {val_loss:.3f} VA {val_acc:.3f}\")\n",
    "        swanlab.log({\"epoch\":ep,\"train_loss\":train_loss,\"train_acc\":train_acc,\"val_loss\":val_loss,\"val_acc\":val_acc,\"lr\":sched.get_last_lr()[0]})\n",
    "\n",
    "        if val_acc > best:\n",
    "            best = val_acc\n",
    "            if use_swa:\n",
    "                torch.save(swa_model.module.state_dict(), \"/workspace/best_swa_model.pth\")\n",
    "            else:\n",
    "                torch.save(model.state_dict(), \"/workspace/best_se_resnet50_mushroom15.pth\")\n",
    "            print(\"  ‚úî Save best\", best)\n",
    "        if estop(val_loss):\n",
    "            print(\"Early stop!\"); break\n",
    "    if use_swa:\n",
    "        torch.optim.swa_utils.update_bn(train_ld, swa_model, device=DEVICE)\n",
    "        torch.save(swa_model.module.state_dict(), \"/workspace/best_swa_model.pth\")\n",
    "        print(\"üì¶ SWA model saved.\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \"/workspace/last_se_resnet50_mushroom15.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
